[
{
	"uri": "https://geometry-of-relativity.net/intro/",
	"title": "Intro to Geometric Algebra",
	"tags": [],
	"description": "",
	"content": "Geometric Algebra is a line of Mathematics which treats vectors as first-class citizens, rather than arrows defined by coordinates on a coordinate system.\nFor a flat Euclidean space, Geometric Algebra can be derived from a single rule that the square of the magnitude of any vector \\(\\mathbf{a}\\) is equal to the square of the vector itself.\nWith this one rule we can introduce addition and multiplication of two-dimensional vectors, as well as a simple rotation.\n\nOne rule The square of the magnitude of an arbitrary vector \\(\\mathbf{a}\\) is equal to the square of the vector itself:\n\\[ |\\mathbf{a}|^2 = \\mathbf{a}^2 \\]\nThis is similar to the more familiar dot-product of a vector with itself defining the Euclidean magnitude-squared of a vector, but as we will see, much more powerful.\nNormal basis vectors Starting with a single dimension, we define a normal basis vector \\(\\mathbf{x}\\) (bold will indicate a vector) as having a magnitude of one:\n\\[ \\mathbf{x}^2 = 1\\]\nAny other vector \\(\\mathbf{a}\\) within this single dimension can be represented as some multiple of \\(\\mathbf{x}\\), such as \\(5\\mathbf{x}\\) or \\(-3.141579\\mathbf{x}\\) or, more generally,\n\\[\\mathbf{a} = x_a\\mathbf{x}\\]\nEvaluating the magnitude-squared of \\(\\mathbf{a}\\) results in:\n\\[ \\mathbf{a}^2 = (x_a\\mathbf{x})^2 = x_a\\mathbf{x}x_a\\mathbf{x} = x_a^2\\mathbf{x}^2 = x_a^2 \\]\nas we would expect.\nOrthogonal basis vectors To describe vectors in two dimensions we need to add a second basis vector \\(y\\) with the similar property of unit-magnitude, so when multiplied by itself,\n\\[ \\mathbf{y}^2 = 1 \\]\nWe also need to ensure that this second vector \\(\\mathbf{y}\\) is orthogonal to \\(\\mathbf{x}\\), though we don't yet know how to represent this orthogonality. Nonetheless, by defining a general two-dimensional vector \\(\\mathbf{a}\\) as\n\\[ \\mathbf{a} = x_a\\mathbf{x} + y_a\\mathbf{y} \\]\nwe can derive orthogonality by using the rule that the magnitude-squared of any vector is equal to the vector squared, to evaluate that:\n\\[ \\begin{aligned} \\mathbf{a}^2 \u0026= (x_a\\mathbf{x} + y_a\\mathbf{y})(x_a\\mathbf{x} + y_a\\mathbf{y}) \\\\ \u0026= x_a\\mathbf{x}x_a\\mathbf{x} + x_a\\mathbf{x}y_a\\mathbf{y} + y_a\\mathbf{y}x_a\\mathbf{x} + y_a\\mathbf{y}y_a\\mathbf{y} \\\\ \u0026= x_a^2 + y_a^2 + x_a y_a(\\mathbf{xy} + \\mathbf{yx}) \\end{aligned} \\]\nSo we see that the square of the magnitude of the vector \\(\\mathbf{a}\\) - a scalar number - will only equal the square of the vector if\n\\[ \\mathbf{xy} = -\\mathbf{yx} \\]\nThis then is the definition of orthogonal vectors - they are anti-commutative.\nWith this definition of orthogonality, we have\n\\[ \\mathbf{a}^2 = x^2 + y^2 \\]\nwhich is exactly what we are used to - Pythagoras' theorem.\nAdding two vectors The algebraic addition of two arbitrary two-dimensional vectors is straight forward,\n\\[ \\begin{aligned} \\mathbf{a} + \\mathbf{b} \u0026= (a_x\\mathbf{x} + a_y\\mathbf{y}) + (b_x\\mathbf{x} + b_y\\mathbf{y}) \\\\ \u0026= (a_x + b_x)\\mathbf{x} + (a_y + b_y)\\mathbf{y} \\\\ \u0026= \\mathbf{b} + \\mathbf{a} \\end{aligned} \\]\nThe order in which the vectors are added is not relevant which means that vectors are associative under addition (and subtraction), as \\(\\mathbf{a} + \\mathbf{b} = \\mathbf{b} + \\mathbf{a}\\).\nActivity: See if you can calculated the magnitude-squared of \\(\\mathbf{a} + \\mathbf{b}\\)\n  Solution   Currently we can only evaluate this as:\n\\[ \\begin{aligned} (\\mathbf{a} + \\mathbf{b})^2 \u0026= (\\mathbf{a} + \\mathbf{b})(\\mathbf{a} + \\mathbf{b}) \\\\ \u0026= \\mathbf{a}^2 + \\mathbf{ab} + \\mathbf{ba} + \\mathbf{b}^2 \\end{aligned} \\]\nbut after the following section on multiplying two vectors, we will be able to see that this is in fact the law of cosines.\n   Multiplying two vectors The geometric product of two vectors may be a little less familiar:\n\\[ \\begin{aligned} \\mathbf{a} \\mathbf{b} \u0026= (a_x\\mathbf{x} + a_y\\mathbf{y})(b_x\\mathbf{x} + b_y\\mathbf{y}) \\\\ \u0026= a_x\\mathbf{x}b_x\\mathbf{x} + a_x\\mathbf{x}b_y\\mathbf{y} + a_y\\mathbf{y}b_x\\mathbf{x} + a_y\\mathbf{y}b_y\\mathbf{y} \\\\ \u0026= a_x b_x + a_y b_y + a_x b_y \\mathbf{xy} + a_y b_x \\mathbf{yx} \\\\ \u0026= a_x b_x + a_y b_y + \\mathbf{xy}(a_x b_y - a_y b_x) \\\\ \\mathbf{ba} \u0026= a_x b_x + a_y b_y - \\mathbf{xy}(a_x b_y - a_y b_x) \\end{aligned} \\]\nThe vector product is not commutative, as \\(\\mathbf{ab} \\neq \\mathbf{ba}\\) nor is it anti-commutative, as \\(\\mathbf{ab} \\neq -\\mathbf{ba}\\). If you are familiar with the traditional vector dot and cross products, you may recognise \\(\\mathbf{ab}\\) as some combination of both, but what is relevant is that it is a combination of a scalar (a pure number) and a pseudo-scalar (the \\(\\mathbf{xy}\\) term, which is not scalar, nor is it a vector).\nBased on the above, we can add or subtract the two products to end up with either a scalar or pseudo-scalar respectively:\n\\[ \\begin{aligned} \\mathbf{ab} + \\mathbf{ba} \u0026= 2(a_x b_x + a_y b_y) \\\\ \\mathbf{ab} - \\mathbf{ba} \u0026= 2\\mathbf{xy}(a_x b_y - a_y b_x) \\end{aligned} \\]\nOf particular interest is that the pseudo-scalar \\(\\mathbf{xy}\\) has the property that:\n\\[ (\\mathbf{xy})^2 = \\mathbf{xyxy} = -\\mathbf{xyyx} = -1 \\]\nRotating vectors in two-dimensions It is worth noting briefly that the product of two vectors, \\(\\mathbf{ab}\\) appears to define a scale and rotation. We will look at this in more detail later, but we can already see that if \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are perpendicular and have a magnitude of 1:\n\\[ \\begin{aligned} \\mathbf{a} \u0026= \\mathbf{x} \\\\ \\mathbf{b} \u0026= \\mathbf{y} \\end{aligned} \\]\nso that\n\\[ \\mathbf{ab} = \\mathbf{xy}\\]\nthen this product will rotate any other vector \\(\\mathbf{c}\\):\n\\[ \\mathbf{c} = c_x\\mathbf{x} + c_y\\mathbf{y} \\]\nby a right-angle each time it is applied:\n\\[ \\begin{aligned} \\mathbf{abc} \u0026= \\mathbf{xy}(c_x\\mathbf{x} + c_y\\mathbf{y}) \\\\ \u0026= c_y\\mathbf{x} - c_x\\mathbf{y} \\\\ \\mathbf{ababc} \u0026= -c_x\\mathbf{x} - c_y\\mathbf{y} = -\\mathbf{c} \\\\ \\mathbf{abababc} \u0026= -c_y\\mathbf{x} + c_x\\mathbf{y} \\\\ \\mathbf{ababababc} \u0026= \\mathbf{c} \\end{aligned} \\]\nBut we will first need to understand Euler's formula to appreciate this rotation in its general form.\n"
},
{
	"uri": "https://geometry-of-relativity.net/",
	"title": "The Geometry of Relativity",
	"tags": [],
	"description": "",
	"content": "The Geometry of Relativity Both relativity and quantum mechanics are often hard to grasp for our minds because they describe physical situations beyond the extremes of our every-day experience.\nThe mathematical tools that we use to describe Special Relativity can be marvelously helpful, but may also unintentionally hide from view a geometric understanding of the Lorentz transformation.\nGeometric Algebra is a relatively recent branch of Mathematics which simplifies many physical processes involving vectors. This site is my attempt to communicate the beauty of geometric algebra as simply as possible while having fun investigating a derivation of the Lorentz transformation as a geometric rotation in space-time, as well as looking at the implications of that geometry.\n Intro to Geometric Algebra  Geometric Algebra is a line of Mathematics which treats vectors as first-class citizens, rather than arrows defined by coordinates on a coordinate system.\nFor a flat Euclidean space, Geometric Algebra can be derived from a single rule that the square of the magnitude of any vector \\(\\mathbf{a}\\) is equal to the square of the vector itself.\nWith this one rule we can introduce addition and multiplication of two-dimensional vectors, as well as a simple rotation.\n\n Rotations in Space  At the end of the previous section we saw that we can rotate an arbitrary vector by \\(\\frac{\\pi}{2}\\) radians simply by multiplying that vector by \\(\\mathbf{xy}.\\) In this section we will see that this is because \\(\\mathbf{xy} = e^{\\mathbf{xy}\\frac{\\pi}{2}}\\) and that more generally we can rotate the same vector by any arbitrary angle \\(\\theta\\) simply by multiplying by \\(e^{\\mathbf{xy}\\theta}\\).\nBut to get to that point we'll first need to take a closer look at Euler's formula and its application to geometric algebra - where we no longer need to define the imaginary unit \\(i\\).\n\n Rotations in Space-Time  Building on rotations using geometric algebra, in this section we define a two-dimensional space-time multivector and then investigate a rotation in space as well as a rotation in space-time, the result of which is equivalent to the Lorentz transformation.  "
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-space/",
	"title": "Rotations in Space",
	"tags": [],
	"description": "",
	"content": "At the end of the previous section we saw that we can rotate an arbitrary vector by \\(\\frac{\\pi}{2}\\) radians simply by multiplying that vector by \\(\\mathbf{xy}.\\) In this section we will see that this is because \\(\\mathbf{xy} = e^{\\mathbf{xy}\\frac{\\pi}{2}}\\) and that more generally we can rotate the same vector by any arbitrary angle \\(\\theta\\) simply by multiplying by \\(e^{\\mathbf{xy}\\theta}\\).\nBut to get to that point we'll first need to take a closer look at Euler's formula and its application to geometric algebra - where we no longer need to define the imaginary unit \\(i\\).\n\n Euler\u0026#39;s Formula  Euler's formula, the most remarkable formula in mathematics according to Richard Feynman, states that for any real number \\(x\\),\n\\[ e^{ix} = \\cos x + i \\sin x \\]\nTo appreciate Euler's formula and it's application to geometric algebra, and ultimately relativity, we need to understand why the imaginary exponent of this special number \\(e\\) results in the trigonometric functions.\nIf you already understand how to derive Euler's formula, you may want to skip over to Euler's formula and Geometric Algebra.  Euler\u0026#39;s Formula and Geometric Algebra  While working to understand and derive Euler's formula we introduced an imaginary unit \\(i\\) with the property that \\(i^2 = -1\\). But we've already seen that the product of the two basis vectors has this same property in that \\( (\\mathbf{xy})^2 = -1 \\).\nIn this section we will investigate the properties of various rotations using Euler's formula and Geometric Algebra in two dimensional space.\n\n 2D Rotations in Space  Rather than representing an arbitrary vector as a combination of two basis vectors: \\(\\mathbf{a} = a_x\\mathbf{x} + a_y\\mathbf{y}\\), in this section we will see that we can simplify the algebra by representing an arbitrary vector as a scale and rotation of a single basis vector: \\(\\mathbf{a} = r_a\\mathbf{x}e^{\\mathbf{xy}\\theta_a}\\) "
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-spacetime/a-vector-in-spacetime/",
	"title": "A Vector in Spacetime",
	"tags": [],
	"description": "",
	"content": "So far we have considered operations on vectors only - linear combinations of the two basis vectors, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Here we will consider a more general combination of a vector together with an \\(\\mathbf{xy}\\) component to form a \u0026quot;multivector\u0026quot;. Furthermore, we will see that this extra component behaves geometrically as we expect time to behave in the space-time of Special Relativity and, by extension, in our every day experience. We will then see that we can rotate these space-time multivectors in space without affecting the \\(\\mathbf{xy}\\) component, corresponding to changing your frame of reference within a single inertial frame.\n At this point we are deviating significantly from the normal understanding of a space-time algebra using Geometric Algebra, where a separate basis vector is normally defined for the time dimension. You have been warned, you may be entering crack-pot territory \n To begin with, let's define an arbitrary two-dimensional space-time multivector as a vector plus a component of \\(\\mathbf{xy}\\):\n\\[ \\mathbf{A} = \\mathbf{a} + t_a\\mathbf{xy} \\]\nThe magnitude-squared of a space-time multivector Multiplying this multivector by itself, we see:\n\\[ \\begin{aligned} \\mathbf{A}^2 \u0026= (\\mathbf{a} + t_a\\mathbf{xy})(\\mathbf{a} + t_a\\mathbf{xy}) \\\\ \u0026= \\mathbf{a}^2 + t_a\\mathbf{axy} + t_a\\mathbf{xya} + t_a^2\\mathbf{xyxy} \\\\ \u0026= \\mathbf{a}^2 + t_a\\mathbf{axy} - t_a\\mathbf{axy} - t_a^2 \\\\ \u0026= \\mathbf{a}^2 - t_a^2 \\\\ \u0026= r_a^2 - t_a^2 \\end{aligned} \\]\nIf you are familiar with Special Relativity, you will recognise this as the Minkowski space-time metric. Note, we have used above some of the earlier properties of rotations to equate \\(\\mathbf{xya} = -\\mathbf{axy}\\), but in full for clarity:\n\\[ \\begin{aligned} \\mathbf{xya} \u0026= \\mathbf{xy}r_a \\mathbf{x} e^{\\mathbf{xy}\\theta_a} \\\\ \u0026= - r_a \\mathbf{xxy} e^{\\mathbf{xy}\\theta_a} \\\\ \u0026= -r_a \\mathbf{x}e^{\\mathbf{xy}\\theta_a}\\mathbf{xy} \\\\ \u0026= -\\mathbf{axy} \\end{aligned} \\]\nSo the square of a space-time multivector results in a scalar value, a value which matches the Minkowski space-time metric.\n\\[ |\\mathbf{A}|^2 = \\mathbf{A}^2 = r_a^2 - t_a^2 \\]\nNormalised Space-Time Multivectors An arbitrary space-time multivector \\(\\mathbf{A}\\) is normal if\n\\[\\mathbf{A}^2 = 1\\]\nUsing the trigonometric identity that\n\\[\\cosh^2\\phi - \\sinh^2\\phi = 1\\]\nthen \\(\\mathbf{A}\\) will be a normalised multivector if \\(\\mathbf{a}^2 = 1\\) and:\n\\[ \\mathbf{A} = \\mathbf{a}\\cosh \\phi + \\mathbf{xy}\\sinh \\phi \\]\nWe can verify this by expanding:\n\\[ \\begin{aligned} \\mathbf{A}^2 \u0026= (\\mathbf{a}\\cosh \\phi + \\mathbf{xy}\\sinh \\phi)(\\mathbf{a}\\cosh \\phi + \\mathbf{xy}\\sinh \\phi) \\\\ \u0026= \\mathbf{a}^2\\cosh^2\\phi + \\mathbf{axy}\\cosh\\phi\\sinh\\phi + \\mathbf{xya}\\cosh\\phi\\sinh\\phi + \\mathbf{xyxy}\\sinh^2\\phi \\\\ \u0026= \\cosh^2\\phi + \\mathbf{axy}\\cosh\\phi\\sinh\\phi - \\mathbf{axy}\\cosh\\phi\\sinh\\phi - \\sinh^2\\phi \\\\ \u0026= 1 \\end{aligned} \\]\nWe can simplify our definition of a normal space-time multivector by looking back to Euler's formula and geometric algebra where we saw that if \\( \\mathbf{a}^2 = 1 \\), then\n\\[ e^{\\mathbf{a}\\phi} = \\cosh\\phi + \\mathbf{a}\\sinh\\phi \\]\nThis allows us to simplify \\(\\mathbf{A}\\) as follows:\n\\[ \\begin{aligned} \\mathbf{A} \u0026= \\mathbf{a}\\cosh\\phi + \\mathbf{xy}\\sinh\\phi \\\\ \u0026= \\mathbf{a}(\\cosh\\phi + \\mathbf{axy}\\sinh\\phi) \u0026 \\text{, since }\\mathbf{a}^2 = 1\\\\ \u0026= \\mathbf{a}e^{\\mathbf{axy}\\phi} \\end{aligned} \\]\nsince \\(\\mathbf{axy}\\) is just the vector \\(\\mathbf{a}\\) rotated by 90 degrees. With this simpler definition of a normal space-time multivector, seeing that it is indeed normal is trivial:\n\\[ \\begin{aligned} \\mathbf{A}^2 \u0026= \\mathbf{a}e^{\\mathbf{axy}\\phi}\\mathbf{a}e^{\\mathbf{axy}\\phi}\\\\ \u0026= \\mathbf{a}e^{\\mathbf{axy}\\phi}e^{-\\mathbf{axy}\\phi}\\mathbf{a} \\\\ \u0026= 1 \\end{aligned} \\]\nTime is unchanged under spatial rotations If we define a spatial rotation using two normalised vectors, \\(\\mathbf{b}\\) and \\(\\mathbf{c}\\), where the difference in orientation between the two is \\(\\theta\\), so that:\n\\[ \\mathbf{bc} = e^{\\mathbf{xy}\\theta} \\]\nwe find that it no longer defines a rotation when simply applied to an arbitrary multivector, because the magnitude-squared after the multiplication is not the same as before:\n\\[ \\begin{aligned} (\\mathbf{Abc})^2 \u0026= \\mathbf{AbcAbc} \\\\ \u0026= (\\mathbf{a} + t_a\\mathbf{xy})\\mathbf{bc}(\\mathbf{a} + t_a\\mathbf{xy})\\mathbf{bc} \\\\ \u0026= (\\mathbf{a} + t_a\\mathbf{xy})(\\mathbf{a}\\mathbf{cb} + t_a\\mathbf{xy}\\mathbf{bc})\\mathbf{bc} \\\\ \u0026= (\\mathbf{a} + t_a\\mathbf{xy})(\\mathbf{a}\\mathbf{cbbc} + t_a\\mathbf{xy}\\mathbf{bcbc}) \\\\ \u0026= (\\mathbf{a} + t_a\\mathbf{xy})(\\mathbf{a} + t_a\\mathbf{xy}e^{\\mathbf{xy}2\\theta}) \\\\ \u0026\\neq \\mathbf{A}^2 \\end{aligned} \\]\nSo instead we halve the angle of rotation, defining:\n\\[ \\mathbf{bc} = e^{\\mathbf{xy}\\frac{\\theta}{2}} \\]\nand apply it twice - once from the left and once from the right: \\(\\mathbf{cbAbc}\\).\nThe fact that \\(\\mathbf{cbAbc}\\) is a rotation can be verified simply by confirming the magnitude-squared is now unchanged:\n\\[ \\begin{aligned} (\\mathbf{cbAbc})^2 \u0026= \\mathbf{cbAbccbAbc} \u0026\\\\ \u0026= \\mathbf{cb}\\mathbf{A}^2\\mathbf{bc} \\\\ \u0026= \\mathbf{A}^2\\mathbf{cbbc} \u0026\\text{, as }\\mathbf{A}^2\\text{is scalar}\\\\ \u0026= \\mathbf{A}^2 \\end{aligned} \\]\nNow, evaluating the actual rotation, we find:\n\\[ \\begin{aligned} \\mathbf{cbAbc} \u0026= \\mathbf{cb}(\\mathbf{a} + t_a\\mathbf{xy})\\mathbf{bc} \\\\ \u0026= \\mathbf{cbabc} + t_a\\mathbf{cbxybc} \\\\ \u0026= \\mathbf{abcbc} + t_a\\mathbf{xycbbc} \\\\ \u0026= \\mathbf{a}e^{\\mathbf{xy}\\theta} + t_a\\mathbf{xy} \\end{aligned} \\]\nThat is, when applying the spatial rotation in this way, the vector component of \\(\\mathbf{A}\\) is rotated by \\(\\theta\\), while the time component of the multi-vector is not affected at all. We can change position and orientation within the one inertial frame without affecting the measured time. Note, we have again used above some of the earlier properties of rotations to equate \\(\\mathbf{cba} = \\mathbf{abc}\\) and \\(\\mathbf{cbxy} = \\mathbf{xycb}\\).\nBut what if, rather than defining a rotation between two spatial vectors, we instead define a rotation between a spatial vector and the pseudo-scalar component \\(\\mathbf{xy}\\) - a two-dimensional rotation in space-time?\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-spacetime/",
	"title": "Rotations in Space-Time",
	"tags": [],
	"description": "",
	"content": "Building on rotations using geometric algebra, in this section we define a two-dimensional space-time multivector and then investigate a rotation in space as well as a rotation in space-time, the result of which is equivalent to the Lorentz transformation.  A Vector in Spacetime  So far we have considered operations on vectors only - linear combinations of the two basis vectors, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Here we will consider a more general combination of a vector together with an \\(\\mathbf{xy}\\) component to form a \u0026quot;multivector\u0026quot;. Furthermore, we will see that this extra component behaves geometrically as we expect time to behave in the space-time of Special Relativity and, by extension, in our every day experience. We will then see that we can rotate these space-time multivectors in space without affecting the \\(\\mathbf{xy}\\) component, corresponding to changing your frame of reference within a single inertial frame.\n The Lorentz Transformation  In the previous section we saw that we can rotate a space-time multivector \\(\\mathbf{A}\\) in space - using a rotation from one normal vector \\(\\mathbf{b}\\) to another normal vector \\(\\mathbf{c}\\) to form the rotation \\(\\mathbf{bc} = e^{\\mathbf{xy}\\frac{\\theta}{2}}\\). We also saw that we can define a normal space-time multivector as \\(\\mathbf{B} = \\mathbf{b}e^{\\mathbf{bxy}\\phi}\\), where \\(\\mathbf{b}^2 = 1\\). In this section we will consider a space-time multivector \\(\\mathbf{A}\\) being rotated in space-time - using a rotation from one normal multivector \\(\\mathbf{B}\\) to another normal multivector \\(\\mathbf{C}\\), and see that the result is the standard Lorentz transformation.\n\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-spacetime/2d-rotations-in-spacetime/",
	"title": "The Lorentz Transformation",
	"tags": [],
	"description": "",
	"content": "In the previous section we saw that we can rotate a space-time multivector \\(\\mathbf{A}\\) in space - using a rotation from one normal vector \\(\\mathbf{b}\\) to another normal vector \\(\\mathbf{c}\\) to form the rotation \\(\\mathbf{bc} = e^{\\mathbf{xy}\\frac{\\theta}{2}}\\). We also saw that we can define a normal space-time multivector as \\(\\mathbf{B} = \\mathbf{b}e^{\\mathbf{bxy}\\phi}\\), where \\(\\mathbf{b}^2 = 1\\). In this section we will consider a space-time multivector \\(\\mathbf{A}\\) being rotated in space-time - using a rotation from one normal multivector \\(\\mathbf{B}\\) to another normal multivector \\(\\mathbf{C}\\), and see that the result is the standard Lorentz transformation.\n\nA simplified Space-Time rotation If we were to do a generic space-time multivector rotation, the rotation would be:\n\\[ \\begin{aligned} \\mathbf{BC} \u0026= \\mathbf{b}e^{\\mathbf{bxy}\\phi_B}\\mathbf{c}e^{\\mathbf{cxy}\\phi_C} \\\\ \\end{aligned} \\]\nbut we are going to simplify this in two ways. First by assuming that the vector component of both \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are equal, and second that they are equal to the basis vector \\(\\mathbf{x}\\). In which case we get a much simplified rotation:\n\\[ \\begin{aligned} \\mathbf{BC} \u0026= \\mathbf{x}e^{\\mathbf{xxy}\\phi_B}\\mathbf{x}e^{\\mathbf{xxy}\\phi_C} \\\\ \u0026= e^{-\\mathbf{y}\\phi_B}\\mathbf{xx}e^{\\mathbf{y}\\phi_C} \\\\ \u0026= e^{\\mathbf{y}(\\phi_C-\\phi_B)} \\\\ \u0026= e^{-\\mathbf{y}\\phi} \\end{aligned} \\]\nwhere \\(\\phi = \\phi_B - \\phi_C\\) is the angle between \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\).\nSimilar to spatial rotations of space-time multivectors, \\(\\mathbf{BC}\\) will not work as a rotation by simple multiplication:\n\\[ (\\mathbf{ABC})^2 = \\mathbf{ABCABC} \\neq \\mathbf{A}^2 \\]\nbut if we left and right multiply then the magnitude-squared remains constant, as it must under a rotation:\n\\[ \\begin{aligned} (\\mathbf{CBABC})^2 \u0026= \\mathbf{CBABCCBABC} \\\\ \u0026= \\mathbf{CBAABC} \\\\ \u0026= \\mathbf{CBBCA}^2 \\\\ \u0026= \\mathbf{A}^2 \\\\ \\end{aligned} \\]\nGiven that we are now applying tho rotation twice, we define \\(\\phi\\) as double the angle between \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\):\n\\[ \\mathbf{BC} = e^{-\\mathbf{y}\\frac{\\phi}{2}} \\]\nwith the result that:\n\\[ \\begin{aligned} \\mathbf{CBABC} \u0026= e^{\\mathbf{y}\\frac{\\phi}{2}}(\\mathbf{a} + t_a\\mathbf{xy})e^{-\\mathbf{y}\\frac{\\phi}{2}} \\\\ \u0026= e^{\\mathbf{y}\\frac{\\phi}{2}}\\mathbf{a}e^{-\\mathbf{y}\\frac{\\phi}{2}} + e^{\\mathbf{y}\\frac{\\phi}{2}}t_a\\mathbf{xy}e^{-\\mathbf{y}\\frac{\\phi}{2}} \\\\ \u0026= e^{\\mathbf{y}\\frac{\\phi}{2}}\\mathbf{a}e^{-\\mathbf{y}\\frac{\\phi}{2}} + t_a\\mathbf{xy}e^{-\\mathbf{y}\\phi} \\end{aligned} \\]\nThe Lorentz Transformation A vector in the direction of rotation If we first look at the case where the vector being rotated is in the same direction as the rotation itself, ie. \\(\\mathbf{a} = x_a\\mathbf{x}\\), we can further simplify the rotation:\n\\( \\begin{aligned} \\mathbf{CBABC} \u0026= e^{\\mathbf{y}\\frac{\\phi}{2}}x_a\\mathbf{x}e^{-\\mathbf{y}\\frac{\\phi}{2}} + t_a\\mathbf{xy}e^{-\\mathbf{y}\\phi} \\\\ \u0026= x_a\\mathbf{x}e^{-\\mathbf{y}\\phi} + t_a\\mathbf{xy}e^{-\\mathbf{y}\\phi} \\\\ \u0026= x_a\\mathbf{x}(\\cosh\\phi - \\mathbf{y}\\sinh\\phi) + t_a\\mathbf{xy}(\\cosh\\phi - \\mathbf{y}\\sinh\\phi) \\\\ \u0026= \\mathbf{x}(x_a\\cosh\\phi - t_a\\sinh\\phi) + \\mathbf{xy}(t_a\\cosh\\phi - x_a\\sinh\\phi) \\\\ \u0026= \\cosh\\phi(x_a - t_a\\tanh\\phi)\\mathbf{x} + \\cosh\\phi(t_a - x_a\\tanh\\phi)\\mathbf{xy} \\end{aligned} \\)  Plotting sinh, cosh and tanh  Wikimedia Commons    Noting that \\(\\tanh\\phi\\) has the property of ranging from -1 to 1 (graph) and that \\(\\cosh\\phi\\) and \\(\\tanh\\phi\\) can be related by:\n\\[ \\cosh\\phi = \\frac{1}{\\sqrt{1-\\tan^2\\phi}} \\]\nSo with the substitution of \\(v = \\tanh\\phi\\), we're left with the Lorentz transformation (with the speed of light ratio normalized to 1):\n\\[ \\mathbf{CBABC} = \\frac{1}{\\sqrt{1-v^2}}(x_a - t_av)\\mathbf{x} + \\frac{1}{\\sqrt{1-v^2}}(t_a - x_av)\\mathbf{xy} \\]\nA vector perpendicular to the direction of rotation Next let us look at the case where the vector being rotated is perpendicular to the rotation, ie. \\(\\mathbf{a} = y_a\\mathbf{y}\\). In this case:\n\\[ \\begin{aligned} \\mathbf{CBABC} \u0026= y_a\\mathbf{y}e^{\\mathbf{y}\\frac{\\phi}{2}}e^{-\\mathbf{y}\\frac{\\phi}{2}} + t_a\\mathbf{xy}e^{-\\mathbf{y}\\phi} \\\\ \u0026= y_a\\mathbf{y} + t_a\\mathbf{xy}(\\cosh\\phi - \\mathbf{y}\\sinh\\phi) \\\\ \u0026= y_a\\mathbf{y} + t_a\\mathbf{xy}\\cosh\\phi - t_a\\mathbf{x}\\sinh\\phi \\\\ \u0026= y_a\\mathbf{y} + \\frac{t_a}{\\sqrt{1-v^2}}\\mathbf{xy} - \\frac{v t_a}{\\sqrt{1-v^2}}\\mathbf{x} \\\\ \\end{aligned} \\]\nSo the component perpendicular to the direction of the rotation is not affected by the transformation, just as with the Lorentz transformation.\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-space/eulers-formula/",
	"title": "Euler&#39;s Formula",
	"tags": [],
	"description": "",
	"content": "Euler's formula, the most remarkable formula in mathematics according to Richard Feynman, states that for any real number \\(x\\),\n\\[ e^{ix} = \\cos x + i \\sin x \\]\nTo appreciate Euler's formula and it's application to geometric algebra, and ultimately relativity, we need to understand why the imaginary exponent of this special number \\(e\\) results in the trigonometric functions.\nIf you already understand how to derive Euler's formula, you may want to skip over to Euler's formula and Geometric Algebra. Finding a function whose derivative is the function itself Euler's formula comes about by first considering a function of \\( x \\), for which the derivative of the function for any value of \\(x\\) has the same value as function itself. That is:\n\\[ \\frac{d}{dx}f(x) = f(x) \\]\nIf we start with a guess of \\( f(x) = 1 + x \\) then we see it is not quite right as\n\\[ \\frac{d}{dx}(1 + x) = 1 \\ne (1 + x) \\]\nbut we can improve it by adding another term\n\\[ f(x) = 1 + x + \\frac{x^2}{2} \\]\nwhich is a little closer in that\n\\[ \\frac{d}{dx}(1 + x + \\frac{x^2}{2}) = 1 + x \\ne f(x) \\]\nbut still not the same. So we improve it again with\n\\[ f(x) = 1 + x + \\frac{x^2}{2} + \\frac{x^3}{2\\times 3}\\]\nand so on, until we end up with an infinite sum of terms:\n\\[ f(x) = \\frac{x^0}{0!} + \\frac{x^1}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + ... = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} \\]\nWe now have a function satisfying \\( \\frac{d}{dx} f(x) = f(x) \\) and can check our solution with:\n\\[ \\begin{aligned} \\frac{d}{dx} f(x) \u0026= \\frac{d}{dx} \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{kx^{k-1}}{k!} \\\\ \u0026= \\sum_{k=1}^{\\infty} \\frac{x^{k-1}}{(k - 1)!} \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} \\\\ \u0026= f(x) \\end{aligned} \\]\nGreat, but where does \\(e\\) fit in?\nUnderstanding what an exponent is We learn in high school that \\( 3^2 \\times 3 = 3^3 \\) and \\( 2^2 \\times 2 \\times 2 = 2^4 \\) and later that \\( x^2 \\times x^2 = x^4 \\) etc., but the general definition of exponentiation is that when multiplying two numbers with the same base (\\( 3^2 \\times 3^1 \\)), the powers (or exponents) are added (\\( 3^{2+1} = 3^3 \\)):\n\\[ b^m \\times b^n = b^{m + n} \\]\nThe definition of e If we take our function which satisfies the property \\(\\frac{d}{dx}f(x) = f(x)\\) we can show that it behaves just like an exponent should, in that:\n\\[ f(m) \\times f(n) = f(m+n) \\]\nTo show this requires quite a bit of re-arranging:\n\\[ \\begin{aligned} f(m) \\times f(n) \u0026= \\sum_{k=0}^{\\infty} \\frac{m^k}{k!} \\times \\sum_{k=0}^{\\infty} \\frac{n^k}{k!} \\\\ \u0026= (\\frac{m^0}{0!} + \\frac{m^1}{1!} + \\frac{m^2}{2!} + \\frac{m^3}{3!} + ...) \\times (\\frac{n^0}{0!} + \\frac{n^1}{1!} + \\frac{n^2}{2!} + \\frac{n^3}{3!} + ...) \\\\ \u0026= 1 + (\\frac{m^1}{1!} + \\frac{n^1}{1!}) + (\\frac{m^2}{2!} + \\frac{m^1 n^1}{1! 1!} + \\frac{n^2}{2!}) + (\\frac{m^3}{3!} + \\frac{m^2 n^1}{2! 1!} + \\frac{m^1 n^2}{1! 2!} + \\frac{n^3}{3!}) ... \\\\ \u0026= 1 + (\\frac{m^1}{1!} + \\frac{n^1}{1!}) + (\\frac{m^2}{2!} + \\frac{2}{2}\\frac{m^1 n^1}{1! 1!} + \\frac{n^2}{2!}) + (\\frac{m^3}{3!} + \\frac{3}{3}\\frac{m^2 n^1}{2! 1!} + \\frac{3}{3}\\frac{m^1 n^2}{1! 2!} + \\frac{n^3}{3!}) ... \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{(m + n)^k}{k!} \\\\ \u0026= f(m + n) \\end{aligned} \\]\nAlso similar to an exponent, we can see that for our function, \\( f(0) = 1\\):\n\\[ \\begin{aligned} f(0) \u0026= \\sum_{k=0}^{\\infty} \\frac{0^k}{k!} \\\\ \u0026= \\frac{0^0}{0!} + \\frac{0^1}{1!} + \\frac{0^2}{2!} + ... \\\\ \u0026= 1 + 0 + 0 + ... \\\\ \u0026= 1 \\end{aligned} \\]\nGiven that our function \\(f(x)\\) behaves just like an exponent should behave, we can define our function simply as a power of some specific number \\(e\\):\n\\[ f(x) = e^x \\]\nand work out what this number \\(e\\) is, with\n\\[ e^1 = f(1) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\approx 2.71828... \\]\nNow any value of our function can be evaluated simply as a power of \\(e\\):\n\\[ f(3) = e^3 = 2.71828... ^3 = 20.08534...\\]\nDerivatives of exponentials What if we need to work with other exponents of \\(e\\), such as \\(e^{3x}\\)? How do we work out the derivative - how fast \\(e^{3x}\\) is changing as \\(x\\) changes? Your memory may (or may not) tell you that:\n\\[ \\frac{d}{dx} e^{3x} = 3e^{3x} \\]\nor more generally, if \\(g\\) is another function of \\(x\\), that\n\\[ \\frac{d}{dx} e^{g} = \\frac{dg}{dx}e^{g} \\]\nThis is all we need for our tooling here, though we can also see why this is the case given the chain rule for derivatives which says that:\n\\[ \\frac{d}{dx}f(g) = \\frac{dg}{dx} \\times \\frac{d}{d g}f(g) \\]\ntogether with the special property of our function \\(f(x) = e^x\\) that the derivative at any point is equal to the function itself, \\(\\frac{d}{dx}f(x) = f(x)\\).\nEuler's formula Now we have enough background to appreciate the beauty of Euler's formula. Euler, like Roger Cotes before him, noticed that if he evaluated this exponent function with a special type of value, an \u0026quot;imaginary\u0026quot; value whose square is negative, the result is a combination of the trigonometric functions \\(\\cos\\) and \\(sin\\). That is, if we create an imaginary unit \\(i\\) with the property that \\(i^2 = -1\\), and evaluate \\(e^{ix}\\):\n\\[ \\begin{aligned} e^{ix} \u0026= \\sum_{k=0}^{\\infty} \\frac{(ix)^k}{k!} \\\\ \u0026= \\frac{(ix)^0}{0!} + \\frac{(ix)^1}{1!} + \\frac{(ix)^2}{2!} + \\frac{(ix)^3}{3!} + \\frac{(ix)^4}{4!} ... \\\\ \u0026= (1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} ...) + i(\\frac{x^1}{1!} - \\frac{x^3}{3!} + \\frac{x^5}{5!} ...) \\\\ \u0026= \\sum_{k=0}^{\\infty}(-1)^{k}\\frac{x^{2k}}{2k!} + i \\sum_{k=0}^{\\infty}(-1)^k\\frac{x^{2k+1}}{(2k+1)!} \\\\ \u0026= \\cos(x) + i\\sin(x) \\end{aligned} \\]\nThat is, the two separate summations are the actual definitions of \\(\\cos(x)\\) and \\(\\sin(x)\\).\nNotice that, with our introduction to Geometric Algebra, we did not need to invent an imaginary unit \\(i\\) here, as we have already seen that the product of basis vectors \\(\\mathbf{xy}\\) has the same property that \\((\\mathbf{xy})^2 = -1\\), but we will pick up this thread in Euler's formula and Geometric Algebra next.\nDerivatives of cos and sin It's worth noting that we can calculate the derivative of \\(e^{ix}\\) and as a result work out the standard derivations of \\(\\cos\\) and \\(sin\\) that we may have memorized in our schooling:\n\\[ \\begin{aligned} \\frac{d}{dx}e^{ix} \u0026= ie^{ix} \\\\ \u0026= i\\cos(x) + i^2\\sin(x) \\\\ \u0026= -\\sin(x) + i\\cos(x) \\end{aligned} \\]\nand so comparing the real parts of \\(e^{ix}\\) and \\(\\frac{d}{dx}e^{ix}\\) show that\n\\[ \\frac{d}{dx}\\cos(x) = -\\sin(x)\\]\nwhile comparing the imaginary parts show that\n\\[ \\frac{d}{dx}\\sin(x) = \\cos(x) \\]\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-space/eulers-formula-and-geometric-algebra/",
	"title": "Euler&#39;s Formula and Geometric Algebra",
	"tags": [],
	"description": "",
	"content": "While working to understand and derive Euler's formula we introduced an imaginary unit \\(i\\) with the property that \\(i^2 = -1\\). But we've already seen that the product of the two basis vectors has this same property in that \\( (\\mathbf{xy})^2 = -1 \\).\nIn this section we will investigate the properties of various rotations using Euler's formula and Geometric Algebra in two dimensional space.\n\nProperties of spatial rotations Rather than introducing the imaginary unit \\(i\\) we could have evaluated the exponent function using the value \\(\\mathbf{xy}\\theta\\) instead. Doing so leads to a similarly periodic result:\n\\[ e^{\\mathbf{xy}\\theta} = \\cos \\theta + \\mathbf{xy}\\sin \\theta \\]\nRather than representing a complex rotation, in this form it represents a rotation in the \\(\\mathbf{xy}\\) plane of our two dimensional space. To see that this is the case, consider multiplying an arbitrary vector \\(\\mathbf{a}\\) by this rotation:\n\\[ \\begin{aligned} \\mathbf{a}e^{\\mathbf{xy}\\theta} \u0026= (a_x\\mathbf{x} + a_y\\mathbf{y})(\\cos \\theta + \\mathbf{xy}\\sin \\theta) \\\\ \u0026= a_x \\cos\\theta \\mathbf{x} + a_x\\mathbf{xxy}\\sin\\theta + a_y\\cos\\theta\\mathbf{y} + a_y\\mathbf{yxy}\\sin\\theta \\\\ \u0026= (a_x \\cos\\theta - a_y\\sin\\theta)\\mathbf{x} + (a_x\\sin\\theta + a_y\\cos\\theta)\\mathbf{y} \\end{aligned} \\]\nwhich matches the normal definition for a two-dimensional rotation of a vector with components \\((a_x, a_y)\\).\nIn the following section, 2d rotations in space, we will use this rotation to simplify the way we represent arbitrary vectors, but for now it will help to note a few properties of this rotation \\(e^{\\mathbf{xy}\\theta}\\).\nFirst, the exponent differs in sign depending on whether you left or right multiply your vector, so:\n\\[ \\mathbf{x}e^{\\mathbf{xy}\\theta} = e^{-\\mathbf{xy}\\theta} \\mathbf{x} \\\\ \\mathbf{y}e^{\\mathbf{xy}\\theta} = e^{-\\mathbf{xy}\\theta} \\mathbf{y} \\]\nand therefore more generally\n\\[ \\mathbf{a}e^{\\mathbf{xy}\\theta} = e^{-\\mathbf{xy}\\theta}\\mathbf{a} \\]\nThis can be seen by expanding to the trigonometric values as follows:\n\\[ \\begin{aligned} \\mathbf{x}e^{\\mathbf{xy}\\theta} \u0026= \\mathbf{x}\\cos \\theta + \\mathbf{xxy}\\sin \\theta \\\\ \u0026= \\mathbf{x} \\cos \\theta -\\mathbf{xyx}\\sin \\theta \\\\ \u0026= (\\cos \\theta -\\mathbf{xy}\\sin \\theta)\\mathbf{x}\\\\ \u0026= e^{-\\mathbf{xy}\\theta} \\mathbf{x} \\end{aligned} \\]\nNext, the exponent does not differ in sign when left or right multiplying by the pseudo-scalar \\(\\mathbf{xy}\\):\n\\[ \\mathbf{xy}e^{\\mathbf{xy}\\theta} = e^{\\mathbf{xy}\\theta} \\mathbf{xy} \\]\nwhich can be seen again by expanding as per the previous example.\nFinally, it's worth noting that, based on the above definition of \\(e^{\\mathbf{xy}\\theta}\\), both \\(\\cos\\theta\\) and \\(\\sin\\theta\\) can be represented as a combination of exponentials as follows:\n\\[ \\cos\\theta = \\frac{e^{\\mathbf{xy}\\theta} + e^{-\\mathbf{xy}\\theta}}{2} \\]\n\\[ \\sin\\theta = \\frac{e^{\\mathbf{xy}\\theta} - e^{-\\mathbf{xy}\\theta}}{2\\mathbf{xy}} \\]\nProperties of hyperbolic rotations We can also investigate using another exponent, such as \\(\\mathbf{x}\\theta\\), which results in something less familiar, but just as significant when looking at the geometry of relativity.\nIn this case, we need to return to the full definition of our exponential function which we derived with Euler's formula, to see how it behaves:\n\\[ \\begin{aligned} e^{\\mathbf{x}\\theta} \u0026= \\sum_{k=0}^{\\infty} \\frac{(\\mathbf{x}\\theta)^k}{k!} \\\\ \u0026= \\frac{(\\mathbf{x}\\theta)^0}{0!} + \\frac{(\\mathbf{x}\\theta)^1}{1!} + \\frac{(\\mathbf{x}\\theta)^2}{2!} + \\frac{(\\mathbf{x}\\theta)^3}{3!} + ... \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{\\theta^{2k}}{2k!} + \\mathbf{x}\\sum_{k=0}^{\\infty} \\frac{\\theta^{2k + 1}}{(2k +1)!} \\end{aligned} \\]\nThe two summations in the last line are one way to define the hyperbolic functions \\(\\cosh\\theta\\) and \\(\\sinh\\theta\\) respectively, so that\n\\[ e^{\\mathbf{x}\\theta} = \\cosh\\theta + \\mathbf{x}\\sinh\\theta \\]\nWe will see later how this hyperbolic rotation becomes important for two-dimensional rotations in space-time but for now, a few analogous but different properties to note, all of which can be seen by expanding \\(e^{\\mathbf{x}\\theta}\\) to the equivalent trigonometric values:\n\\[\\mathbf{x}e^{\\mathbf{x}\\theta} = e^{\\mathbf{x}\\theta}\\mathbf{x}\\]\n\\[\\mathbf{y}e^{\\mathbf{x}\\theta} = e^{-\\mathbf{x}\\theta}\\mathbf{y}\\]\n\\[\\mathbf{x}e^{\\mathbf{y}\\theta} = e^{-\\mathbf{y}\\theta}\\mathbf{x}\\]\n\\[\\mathbf{y}e^{\\mathbf{y}\\theta} = e^{\\mathbf{y}\\theta}\\mathbf{y}\\]\n\\[\\mathbf{xy}e^{\\mathbf{x}\\theta} = e^{-\\mathbf{x}\\theta}\\mathbf{xy}\\]\n\\[\\mathbf{xy}e^{\\mathbf{y}\\theta} = e^{-\\mathbf{y}\\theta}\\mathbf{xy}\\]\n\\[ \\cosh\\theta = \\frac{e^{\\mathbf{x}\\theta} + e^{-\\mathbf{x}\\theta}}{2} \\]\n\\[ \\sinh\\theta = \\frac{e^{\\mathbf{x}\\theta} - e^{-\\mathbf{x}\\theta}}{2\\mathbf{x}} \\]\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-space/2d-rotations-in-space/",
	"title": "2D Rotations in Space",
	"tags": [],
	"description": "",
	"content": "Rather than representing an arbitrary vector as a combination of two basis vectors: \\(\\mathbf{a} = a_x\\mathbf{x} + a_y\\mathbf{y}\\), in this section we will see that we can simplify the algebra by representing an arbitrary vector as a scale and rotation of a single basis vector: \\(\\mathbf{a} = r_a\\mathbf{x}e^{\\mathbf{xy}\\theta_a}\\) Summary so far A basis for Geometric Algebra in two dimensions In the introduction to Geometric Algebra we saw that we can define a 2D space using the basis vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), where each basis vector's magnitude-squared is 1:\n\\[ \\mathbf{x}^2 = \\mathbf{y}^2 = 1 \\]\nand where each basis vector is orthogonal to the other, which we represented by their anti-commutativity:\n\\[ \\mathbf{x}\\mathbf{y} = -\\mathbf{y}\\mathbf{x} \\]\nso that any 2D vector constructed as a combination of these basis vectors:\n\\[ \\mathbf{a} = x_a\\mathbf{x} + y_a\\mathbf{y} \\]\nstill satisfies the requirement that the magnitude-squared of the vector is simply the square of the vector: \\(|\\mathbf{a}|^2 = \\mathbf{a}^2\\).\nUsing Euler's formula to rotate vectors in two dimensions After some background on Euler's formula itself, we then investigated combining Euler's formula with Geometric Algebra, to see that\n\\[ e^{\\mathbf{xy}\\theta} = \\cos\\theta + \\mathbf{xy}\\sin\\theta \\]\ndefines a rotation in two-dimensional space, which can be applied to an arbitrary vector to rotate it by \\(\\theta\\) in the \\(\\mathbf{xy}\\) plane:\n\\[ \\mathbf{a}e^{\\mathbf{xy}\\theta} \\]\nRepresenting vectors as scaled rotations of a basis This rotation provided by Euler's formula also allows us another way to represent arbitrary vectors. So far we have been representing an arbitrary vector \\(\\mathbf{a}\\) as a combination of the two basis vectors for the two-dimensional space:\n\\[ \\mathbf{a} = x_a\\mathbf{x} + y_a\\mathbf{y} \\]\nBut we can now additionally represent an arbitrary vector as a magnitude and orientation applied to a basis vector:\n\\[ \\mathbf{a} = r_a \\mathbf{x}e^{\\mathbf{xy}\\theta_a} = r_a e^{-\\mathbf{xy}\\theta_a}\\mathbf{x} \\]\nwhere \\(r_a\\) is the magnitude and \\(\\theta_a\\) the orientation of \\(\\mathbf{a}\\).\nProperties Using the properties of spatial rotations which we noted earlier, this allows more intuitive results, such as the magnitude-squared of a vector:\n\\[ \\begin{aligned} \\mathbf{a}^2 \u0026= (r_a e^{-\\mathbf{xy}\\theta_a}\\mathbf{x})(r_a e^{-\\mathbf{x}\\mathbf{y}\\theta_a}\\mathbf{x}) \\\\ \u0026= r_a^2 e^{-\\mathbf{xy}\\theta_a} e^{\\mathbf{xy}\\theta_a}\\mathbf{xx} \\\\ \u0026= r_a^2 \\end{aligned} \\]\nor the product of two vectors:\n\\[ \\begin{aligned} \\mathbf{a}\\mathbf{b} \u0026= r_a r_b \\mathbf{x}e^{\\mathbf{x}\\mathbf{y}\\theta_a} \\mathbf{x}e^{\\mathbf{x}\\mathbf{y}\\theta_b} \\\\ \u0026= r_a r_b \\mathbf{x}^2 e^{-\\mathbf{x}\\mathbf{y}\\theta_a} e^{\\mathbf{x}\\mathbf{y}\\theta_b} \\\\ \u0026= r_a r_b e^{\\mathbf{x}\\mathbf{y}(\\theta_b -\\theta_a)} \\end{aligned} \\]\nThis product of two vectors is now more intuitively recognised, without expanding, as a scale (\\(r_a r_b\\)) and rotation in the \\(\\mathbf{x}\\mathbf{y}\\) plane by \\(\\theta_b - \\theta_a\\) which we will apply in the next section.\nThis also makes other properties more intuitive, such as\n\\[ \\begin{aligned} \\mathbf{ab} + \\mathbf{ba} \u0026= r_a r_b e^{\\mathbf{x}\\mathbf{y}(\\theta_b -\\theta_a)} + r_a r_b e^{\\mathbf{x}\\mathbf{y}(\\theta_a -\\theta_b)} \\\\ \u0026= r_a r_b (e^{\\mathbf{x}\\mathbf{y}(\\theta_b -\\theta_a)} + e^{-\\mathbf{x}\\mathbf{y}(\\theta_b -\\theta_a)}) \\\\ \u0026= 2 r_a r_b \\cos (\\theta_b - \\theta_a) \\end{aligned} \\]\nand\n\\[ \\begin{aligned} \\mathbf{ab} - \\mathbf{ba} \u0026= r_a r_b e^{\\mathbf{x}\\mathbf{y}(\\theta_b -\\theta_a)} - r_a r_b e^{\\mathbf{x}\\mathbf{y}(\\theta_a -\\theta_b)} \\\\ \u0026= r_a r_b (e^{\\mathbf{x}\\mathbf{y}(\\theta_b -\\theta_a)} - e^{-\\mathbf{x}\\mathbf{y}(\\theta_b -\\theta_a)}) \\\\ \u0026= 2 r_a r_b \\sin (\\theta_b - \\theta_a) \\end{aligned} \\]\nwhich in turn allows a very simple derivation of the cosine rule for the magnitude of the third side of an arbitrary triangle given two sides:\n\\[ \\begin{aligned} (\\mathbf{a} + \\mathbf{b})^2 \u0026= \\mathbf{a}^2 + \\mathbf{ab} + \\mathbf{ba} + \\mathbf{b}^2 \\\\ \u0026= r_a^2 + r_b^2 + 2r_a r_b \\cos(\\theta_b - \\theta_a) \\\\ \u0026= r_a^2 + r_b^2 - 2r_a r_b \\cos(\\pi -(\\theta_b - \\theta_a)) \\end{aligned} \\]\nRotating a vector in two dimensions With this definition of arbitrary vectors, it is much easier to see that the product of two vectors defines a scale and rotation, which can be applied to a third vector:\n\\[ \\begin{aligned} \\mathbf{abc} \u0026= r_a\\mathbf{x}e^{\\mathbf{xy}\\theta_a}r_b\\mathbf{x}e^{\\mathbf{xy}\\theta_b}\\mathbf{c} \\\\ \u0026= r_a r_b e^{-\\mathbf{xy}\\theta_a}e^{\\mathbf{xy}\\theta_b}\\mathbf{c} \\\\ \u0026= r_a r_b e^{\\mathbf{xy}(\\theta_b - \\theta_a)}\\mathbf{c} \\end{aligned} \\]\nWe can see this is in fact a scale and rotation if we also expand \\(\\mathbf{c}\\)\n\\[ \\begin{aligned} \\mathbf{abc} \u0026= r_a r_b e^{\\mathbf{xy}(\\theta_b - \\theta_a)}r_c\\mathbf{x}e^{\\mathbf{xy}\\theta_c} \\\\ \u0026= r_a r_b r_c \\mathbf{x}e^{\\mathbf{xy}(\\theta_c - (\\theta_b - \\theta_a))} \\end{aligned} \\]\nshowing that \\(\\mathbf{c}\\) is scaled by a factor of \\(r_a r_b\\) and rotated by an amount \\(\\theta_b - \\theta_a\\).\nThis can also be viewed as the vector \\(\\mathbf{a}\\) being scaled by an amount \\(r_b r_c\\) and rotated by an amount \\(\\theta_b - \\theta_c\\), as we can rearrange the above to the equivalent:\n\\[ \\mathbf{abc} = r_a r_b r_c \\mathbf{x}e^{\\mathbf{xy}(\\theta_a - (\\theta_b - \\theta_c))} \\]\nSo we can represent a vector in two dimensional space. We can add, scale and rotate such vectors. But is there a way to inherently represent a vector in space-time?\n"
},
{
	"uri": "https://geometry-of-relativity.net/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://geometry-of-relativity.net/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "Contributors Thanks to them  for helping to spread the love of Geometric Algebra by having fun playing with its application in physics. Help others learn and get your contributions listed here! We need help creating visualisations, correcting and improving the text, adding activities where helpful, extending the text with testable implications, ...\n.ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }   @absoludity 13 commits   "
},
{
	"uri": "https://geometry-of-relativity.net/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]